<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
<LINK REL="stylesheet" HREF="../../../style.css">
 <META NAME="GENERATOR" CONTENT="LinuxDoc-Tools 0.9.73">
 <TITLE>DOMjudge Jury Manual: Before the contest</TITLE>
 <LINK HREF="judge-manual-4.html" REL=next>
 <LINK HREF="judge-manual-2.html" REL=previous>
 <LINK HREF="judge-manual.html#toc3" REL=contents>
</HEAD>
<BODY>
<A HREF="judge-manual-4.html">Next</A>
<A HREF="judge-manual-2.html">Previous</A>
<A HREF="judge-manual.html#toc3">Contents</A>
<HR>
<H2><A NAME="s3">3.</A> <A HREF="judge-manual.html#toc3">Before the contest</A></H2>


<P>Before the contest starts, a number of things will need to be
configured by the administrator. You can check that information,
such as the problem set(s), test data and time limits, contest
start- and end time, the time at which the scoreboard will be
frozen and unfrozen, all from the links from the front page.</P>
<P>Note that multiple contests can be defined, with corresponding
problem sets, for example a practice session and the real contest.</P>

<H2><A NAME="ss3.1">3.1</A> <A HREF="judge-manual.html#toc3.1">Problems and languages</A>
</H2>


<P>The problem sets are listed under `Problems'. It is possible to change
whether teams can submit solutions for that problem (using the toggle
switch `allow submit'). If disallowed, submissions for that problem
will be rejected, but more importantly, teams will not see that
problem on the scoreboard. Disallow judge will make DOMjudge accept
submissions, but leave them queued; this is useful in case an
unexpected problem shows up with one of the problems. Timelimit is the
maximum number of seconds a submission for this problem is allowed to
run before a `TIMELIMIT' response is given (to be multiplied possibly
by a language factor). Note that a `timelimit overshoot' can be
configured to let submissions run a bit longer. Although DOMjudge will
use the actual limit to determine the verdict, this allows judges to
see if a submission is close to the timelimit.</P>
<P>Problems can be imported and exported into and from DOMjudge
using zip-files that contain the problem metadata and testdata files,
based on the <CODE>problemarchive.org</CODE> format.
See appendix 
<A HREF="judge-manual-6.html#problem-format">Problem package format -specification</A> for details.
Problems can have special <EM>compare</EM> and
<EM>run</EM> scripts associated to them, to deal with problem
statements that require non-standard evaluation. For more details see
the administrator's manual.</P>
<P>The `Languages' overview is quite the same. It has a timefactor
column; submissions in a language that has time factor 2 will be
allowed to run twice the time that has been specified under Problems.
This can be used to compensate for the execution speed of a language,
e.g. Java.</P>

<H2><A NAME="ss3.2">3.2</A> <A HREF="judge-manual.html#toc3.2">Verifying testdata</A>
</H2>


<P>For checking whether the your testdata conforms to the specifications
of your problem statement, we recommend the checktestdata program,
which is available from a 
<A HREF="https://github.com/DOMjudge/checktestdata">separate repository</A>.
It allows you to not only check on simple (spacing) layout errors, but
a simple grammar file must be specified for the testdata, according to
which the testdata is checked. This allows e.g. for bounds checking.</P>
<P>This program is built upon the separate library <CODE>libchecktestdata.h</CODE>
that can be used to write the syntax checking part of special
compare scripts: it can easily handle the tedious task of verifying
that a team's submission output is syntactically valid, leaving just
the task of semantic validation to another program.</P>

<H2><A NAME="ss3.3">3.3</A> <A HREF="judge-manual.html#toc3.3">Testing jury solutions</A>
</H2>


<P>Before a contest, you will want to have tested your reference
solutions on the system to see whether those are judged as expected
and maybe use their runtimes to set timelimits for the problems.</P>
<P>The simplest way to do this is to include the jury solutions in a
problem zip file and upload this. You can also upload a zip file
containing just solutions to an existing problem. Note that the zip
archive has to adhere to the 
<A HREF="http://www.problemarchive.org/wiki/index.php/Problem_Format">Kattis problem package format</A>.
For this to work, the jury/admin who uploads the problem has to have
an associated team to which the solutions will be assigned. The
solutions will automatically be judged if the contest is active (but
it need not have started yet). You can verify whether the submissions
gave the expected answer from the link on the jury/admin index page.</P>

<H2><A NAME="ss3.4">3.4</A> <A HREF="judge-manual.html#toc3.4">Practice Session</A>
</H2>


<P>If your contest has a test session or practice contest, use it also as
a general rehearsal of the jury system: judge test submissions as you
would do during the real contest and answer incoming clarification
requests.</P>


<HR>
<A HREF="judge-manual-4.html">Next</A>
<A HREF="judge-manual-2.html">Previous</A>
<A HREF="judge-manual.html#toc3">Contents</A>
</BODY>
</HTML>
